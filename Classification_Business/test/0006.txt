自然言語処理にあまり馴染みがないのもあって、試しに TF-IDF (Term Frequency - Inverse Document Frequency) を自分で実装してみることにした。 その過程で知ったことについて書き残しておく。 端的に書いてしまうと、TF-IDF の定義は色々とある、ということ。

TF-IDF というのは、コーパス (全文書) に含まれる単語の重要度を評価するための手法。 その名の通り、文書単位で見た単語の頻度 (Term Frequency) と、コーパス単位で見た単語の頻度 (Inverse Document Frequency) を元に計算する。 これは例えば、ある文書の中で頻出する単語 (Term Frequency が高い) があれば、その単語はその文書の中では重要と考えられる。 しかし、その単語が別の文書でも同様によく登場する (Inverse Document Frequency が低い) のであれば、全体から見るとありふれた単語なので実は大して重要ではなくなる。

今回、TF-IDF を実装するにあたって scikit-learn の出力結果をお手本にすることにした。 しかし、やってみるとなかなか結果が一致しない。 原因を探るべくドキュメントやソースコードやチケットシステムを確認していくと、色々と事情が分かってきた。

動作を確認するのに使った環境は次の通り。